{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offensive Language Classification\n",
    "This project aims at identifying whether the offensive language in the tweet is targeted towards a person, group, or organization (SUBTASK B)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading dataset into Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>97670</td>\n",
       "      <td>@USER Liberals are all Kookoo !!!</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>OTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>77444</td>\n",
       "      <td>@USER @USER Oh noes! Tough shit.</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>52415</td>\n",
       "      <td>@USER was literally just talking about this lo...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>GRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>45157</td>\n",
       "      <td>@USER Buy more icecream!!!</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>13384</td>\n",
       "      <td>@USER Canada doesn’t need another CUCK! We alr...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet subtask_a  \\\n",
       "0  86426  @USER She should ask a few native Americans wh...       OFF   \n",
       "1  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...       OFF   \n",
       "2  16820  Amazon is investigating Chinese employees who ...       NOT   \n",
       "3  62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n",
       "4  43605  @USER @USER Obama wanted liberals &amp; illega...       NOT   \n",
       "5  97670                  @USER Liberals are all Kookoo !!!       OFF   \n",
       "6  77444                   @USER @USER Oh noes! Tough shit.       OFF   \n",
       "7  52415  @USER was literally just talking about this lo...       OFF   \n",
       "8  45157                         @USER Buy more icecream!!!       NOT   \n",
       "9  13384  @USER Canada doesn’t need another CUCK! We alr...       OFF   \n",
       "\n",
       "  subtask_b subtask_c  \n",
       "0       UNT      NULL  \n",
       "1       TIN       IND  \n",
       "2      NULL      NULL  \n",
       "3       UNT      NULL  \n",
       "4      NULL      NULL  \n",
       "5       TIN       OTH  \n",
       "6       UNT      NULL  \n",
       "7       TIN       GRP  \n",
       "8      NULL      NULL  \n",
       "9       TIN       IND  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('C:/Users/wjaya/Downloads/OLIDv1.0/olid-training-v1.0.tsv', sep = '\\t')\n",
    "\n",
    "data = data.fillna('NULL')\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating sub-functions to perform data-processing:\n",
    "The sub-functions aim to:\n",
    "- remove Tagged users, URLS, and ampersands\n",
    "- get the Part-of-Speech tag for words within the tweet\n",
    "- tokenize and lemmatize tweets\n",
    "- count number of users tagged in a tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() \n",
    "stops = stopwords.words('english')\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "def script_preprocessing(df):\n",
    "    \n",
    "    def removewords(text): # Remove these words.\n",
    "        \n",
    "        text = text.replace('@USER','',50)\n",
    "        text = text.strip('URL')\n",
    "        text = text.replace('&amp','',10)\n",
    "        return text\n",
    "\n",
    "    def get_wordnet_pos(tag):\n",
    "\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "    \n",
    "    def pos_lemma(text):\n",
    "        \n",
    "        tokens = [i for i in tokenizer.tokenize(str(removewords(text)).lower()) if i not in stops] # if we don't convert text to str, it rises TypeError: expected string or bytes-like object   \n",
    "        tagged = pos_tag(tokens)\n",
    "        lemlist = [lemmatizer.lemmatize(i[0], get_wordnet_pos(i[1])) for i in tagged]\n",
    "        lemmas = ' '.join(lemlist).lower()\n",
    "\n",
    "        return lemmas\n",
    "    \n",
    "    def countuser(text):\n",
    "        splitted_text = text.lower().split()\n",
    "        user_count = 0\n",
    "        for word in splitted_text \n",
    "            word = re.sub(\"[#@]\",\"\",word)\n",
    "            word = re.sub(\"!\",\" !\",word)\n",
    "            word = re.sub(\"[?]\",\" ?\",word)\n",
    "            if(word == 'user'):\n",
    "                user_count += 1\n",
    "        return user_count\n",
    "                        \n",
    "    def finalize(df):\n",
    "        \n",
    "        df['pos_lemmatized'] = [pos_lemma(i) for i in df['tweet']]\n",
    "        df['user_count'] = [countuser(i) for i in df['tweet']]\n",
    "        return df\n",
    "        \n",
    "    return finalize(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "      <th>pos_lemmatized</th>\n",
       "      <th>user_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NULL</td>\n",
       "      <td>ask native american take</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "      <td>go home drunk maga trump2020</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>amazon investigate chinese employee sell inter...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NULL</td>\n",
       "      <td>someone vetaken piece shit volcano</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>obama want liberal illegals move red state</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet subtask_a  \\\n",
       "0  86426  @USER She should ask a few native Americans wh...       OFF   \n",
       "1  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...       OFF   \n",
       "2  16820  Amazon is investigating Chinese employees who ...       NOT   \n",
       "3  62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n",
       "4  43605  @USER @USER Obama wanted liberals &amp; illega...       NOT   \n",
       "\n",
       "  subtask_b subtask_c                                     pos_lemmatized  \\\n",
       "0       UNT      NULL                           ask native american take   \n",
       "1       TIN       IND                       go home drunk maga trump2020   \n",
       "2      NULL      NULL  amazon investigate chinese employee sell inter...   \n",
       "3       UNT      NULL                 someone vetaken piece shit volcano   \n",
       "4      NULL      NULL         obama want liberal illegals move red state   \n",
       "\n",
       "   user_count  \n",
       "0           1  \n",
       "1           3  \n",
       "2           0  \n",
       "3           1  \n",
       "4           2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = script_preprocessing(data)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "      <th>pos_lemmatized</th>\n",
       "      <th>user_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NULL</td>\n",
       "      <td>ask native american take</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "      <td>go home drunk maga trump2020</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>amazon investigate chinese employee sell inter...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NULL</td>\n",
       "      <td>someone vetaken piece shit volcano</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>obama want liberal illegals move red state</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet subtask_a  \\\n",
       "0  86426  @USER She should ask a few native Americans wh...       OFF   \n",
       "1  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...       OFF   \n",
       "2  16820  Amazon is investigating Chinese employees who ...       NOT   \n",
       "3  62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n",
       "4  43605  @USER @USER Obama wanted liberals &amp; illega...       NOT   \n",
       "\n",
       "  subtask_b subtask_c                                     pos_lemmatized  \\\n",
       "0       UNT      NULL                           ask native american take   \n",
       "1       TIN       IND                       go home drunk maga trump2020   \n",
       "2      NULL      NULL  amazon investigate chinese employee sell inter...   \n",
       "3       UNT      NULL                 someone vetaken piece shit volcano   \n",
       "4      NULL      NULL         obama want liberal illegals move red state   \n",
       "\n",
       "   user_count  \n",
       "0        0.02  \n",
       "1        0.06  \n",
       "2        0.00  \n",
       "3        0.02  \n",
       "4        0.04  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating ratio for number of users tagged in a post as number of users in the post vs maximum number of users in a post\n",
    "max_ = result['user_count'].max()\n",
    "result['user_count'] = result['user_count'].apply(lambda x:x/max_)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "      <th>pos_lemmatized</th>\n",
       "      <th>user_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NULL</td>\n",
       "      <td>ask native american take</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "      <td>go home drunk maga trump2020</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NULL</td>\n",
       "      <td>someone vetaken piece shit volcano</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>97670</td>\n",
       "      <td>@USER Liberals are all Kookoo !!!</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>OTH</td>\n",
       "      <td>liberal kookoo</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>77444</td>\n",
       "      <td>@USER @USER Oh noes! Tough shit.</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NULL</td>\n",
       "      <td>oh no tough shit</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet subtask_a  \\\n",
       "0  86426  @USER She should ask a few native Americans wh...       OFF   \n",
       "1  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...       OFF   \n",
       "3  62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n",
       "5  97670                  @USER Liberals are all Kookoo !!!       OFF   \n",
       "6  77444                   @USER @USER Oh noes! Tough shit.       OFF   \n",
       "\n",
       "  subtask_b subtask_c                      pos_lemmatized  user_count  \n",
       "0       UNT      NULL            ask native american take        0.02  \n",
       "1       TIN       IND        go home drunk maga trump2020        0.06  \n",
       "3       UNT      NULL  someone vetaken piece shit volcano        0.02  \n",
       "5       TIN       OTH                      liberal kookoo        0.02  \n",
       "6       UNT      NULL                    oh no tough shit        0.04  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing rows having a NULL value for the target variable\n",
    "ind = []\n",
    "for i in range(len(result['subtask_b'])):\n",
    "    if result['subtask_b'][i] == 'NULL':\n",
    "        ind.append(i)\n",
    "result.drop(axis=0,labels = ind,inplace = True)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataframes for X and y:\n",
    "X contains the POS Lemmatized tweets and y contains the target column subtask_b which is 'UNT' when untargeted and 'TIN' when the tweet is targeted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = result[['pos_lemmatized',\"user_count\"]]\n",
    "y = result['subtask_b']\n",
    "label_to_number = {'UNT':0,'TIN':1}\n",
    "number_to_label = {v:k for k,v in label_to_number.items()}\n",
    "Y = result.subtask_b.apply(lambda x:label_to_number[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a function to upsample and downsample the data to reduce class-imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_size_data(X_train, count_train, y_train, ratio_down_over_up=0.5):\n",
    "    X_train = list(X_train)\n",
    "    count_train = list(count_train)\n",
    "    y_train = list(y_train)\n",
    "  \n",
    "    n_cat = len(Counter(y_train))\n",
    "  \n",
    "    sorted_counter = Counter(y_train).most_common()\n",
    "    max_cat = sorted_counter[0][1]\n",
    "    min_cat = sorted_counter[-1][1]\n",
    "\n",
    "    target = min_cat + (1-ratio_down_over_up)*(max_cat - min_cat)\n",
    "\n",
    "    for i in range(n_cat):\n",
    "        diff = int(sorted_counter[i][1] - target)\n",
    "        k = 0\n",
    "        if diff > 0:\n",
    "            rm = 0    \n",
    "            while rm <= diff:\n",
    "                if(y_train[k] == sorted_counter[i][0]):\n",
    "                    X_train.pop(k)\n",
    "                    y_train.pop(k)\n",
    "                    count_train.pop(k)\n",
    "                    rm += 1\n",
    "                    k -=1\n",
    "                k += 1\n",
    "        else:\n",
    "            ad = 0\n",
    "            while ad <= -diff:\n",
    "                if(y_train[k] == sorted_counter[i][0]):\n",
    "                    X_train.append(X_train[k])\n",
    "                    y_train.append(y_train[k])\n",
    "                    count_train.append(count_train[k])\n",
    "                    ad += 1\n",
    "                k += 1\n",
    "\n",
    "    return X_train, count_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into training dataset and testing dataset and performing shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_b, count_train_b, y_train_b = same_size_data(X.pos_lemmatized, X.user_count, Y, 0.2)\n",
    "X_train_b, count_train_b, y_train_b = shuffle(X_train_b, count_train_b, y_train_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a bag of words model taking in the top 5000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['liberal', 'gun', 'like', 'control', 'get', 'people', 'go', 'shit', 'antifa', 'say', 'fuck', 'maga', 'know', 'conservative', 'think', 'trump', 'one', 'make', 'u', 'want', 'right', 'need', 'good', 'woman', 'would', 'democrat', 'lie', 'see', 'time', 'take']\n"
     ]
    }
   ],
   "source": [
    "# Bag of words model.\n",
    "all_words_list = []\n",
    "for sent in data['pos_lemmatized']:\n",
    "    for word in sent.split(' '):\n",
    "        all_words_list.append(word)\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "word_items = all_words.most_common(5000)\n",
    "\n",
    "word_features = [word for (word,count) in word_items]\n",
    "print(word_features[:30])\n",
    "\n",
    "#word_features = tokenizer.texts_to_sequences(word_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TF-IDF Vectorizer to obtain TF-IDF for each word in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def TFIDFmatrix(X_train,word_features):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    fit = vectorizer.fit(word_features)\n",
    "    X = vectorizer.transform(X_train)\n",
    "    X = X.todense()\n",
    "    return X\n",
    "    \n",
    "X_train_b = TFIDFmatrix(X_train_b,word_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a function to perform 5-fold cross validation for models implemented and print its respective model performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(X,y,clf):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state = 52)\n",
    "    iteration_index = 0\n",
    "    acc_list_scores = []\n",
    "    f1_list_scores = []\n",
    "    pre_list_scores=[]\n",
    "    re_list_scores=[]\n",
    "    for train_indexes, test_indexes in kf.split(X, y):\n",
    "        iteration_index += 1\n",
    "\n",
    "        X_train = X[train_indexes]\n",
    "        y_train = y[train_indexes]\n",
    "\n",
    "        X_test = X[test_indexes]\n",
    "        y_test = y[test_indexes]\n",
    "\n",
    "        #logreg = linear_model.LogisticRegression(C=1e5)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_predict = logreg.predict(X_test)\n",
    "\n",
    "        current_acc = accuracy_score(y_test, y_predict)\n",
    "        current_pre=precision_score(y_test,y_predict,average='macro',labels=np.unique(y_predict))\n",
    "        current_recall=recall_score(y_test, y_predict,average='macro',labels=np.unique(y_predict))    \n",
    "        current_f1 = f1_score(y_test, y_predict, average='macro', labels=np.unique(y_predict))\n",
    "        \n",
    "  \n",
    "        print(\"Iteration #{0}: Accuracy : {1}, Precision : {2}, Recall : {2}, F-score : {2}\".format(\n",
    "            iteration_index,current_acc, current_pre,current_recall,current_f1))\n",
    "        acc_list_scores.append(current_acc)\n",
    "        pre_list_scores.append(current_pre)\n",
    "        re_list_scores.append(current_recall)\n",
    "        f1_list_scores.append(current_f1)\n",
    "        \n",
    "\n",
    "    print(\"Accuracy: {0}\".format(np.mean(acc_list_scores)))\n",
    "    print(\"Precision: {0}\".format(np.mean(pre_list_scores)))\n",
    "    print(\"Recall: {0}\".format(np.mean(re_list_scores)))\n",
    "    print(\"F1-measure: {0}\".format(np.mean(f1_list_scores)))\n",
    "    \n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #1: Accuracy : 0.8908807482462977, Precision : 0.9091973798236224, Recall : 0.9091973798236224, F-score : 0.9091973798236224\n",
      "Iteration #2: Accuracy : 0.8845553822152886, Precision : 0.9002695417789757, Recall : 0.9002695417789757, F-score : 0.9002695417789757\n",
      "Iteration #3: Accuracy : 0.8806552262090483, Precision : 0.9056720098643649, Recall : 0.9056720098643649, F-score : 0.9056720098643649\n",
      "Iteration #4: Accuracy : 0.8884555382215289, Precision : 0.9046080557843796, Recall : 0.9046080557843796, F-score : 0.9046080557843796\n",
      "Iteration #5: Accuracy : 0.9024960998439937, Precision : 0.9227441285537701, Recall : 0.9227441285537701, F-score : 0.9227441285537701\n",
      "Accuracy: 0.8894085989472315\n",
      "Precision: 0.9084982231610225\n",
      "Recall: 0.8893290249394947\n",
      "F1-measure: 0.8878918502457024\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "logreg = linear_model.LogisticRegression(C=1e5, solver='liblinear')\n",
    "LRmodel = training(X_train_b,np.array(y_train_b),logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #1: Accuracy : 0.9883086515978177, Precision : 0.9886535552193646, Recall : 0.9886535552193646, F-score : 0.9886535552193646\n",
      "Iteration #2: Accuracy : 0.9914196567862714, Precision : 0.990909090909091, Recall : 0.990909090909091, F-score : 0.990909090909091\n",
      "Iteration #3: Accuracy : 0.9867394695787831, Precision : 0.9874074074074074, Recall : 0.9874074074074074, F-score : 0.9874074074074074\n",
      "Iteration #4: Accuracy : 0.9906396255850234, Precision : 0.9905660377358491, Recall : 0.9905660377358491, F-score : 0.9905660377358491\n",
      "Iteration #5: Accuracy : 0.9024960998439937, Precision : 0.9227441285537701, Recall : 0.9227441285537701, F-score : 0.9227441285537701\n",
      "Accuracy: 0.9719207006783778\n",
      "Precision: 0.9760560439650965\n",
      "Recall: 0.970595297507133\n",
      "F1-measure: 0.9713649438261728\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB()\n",
    "NBmodel = training(X_train_b,np.array(y_train_b),nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #1: Accuracy : 0.9883086515978177, Precision : 0.9886535552193646, Recall : 0.9886535552193646, F-score : 0.9886535552193646\n",
      "Iteration #2: Accuracy : 0.9914196567862714, Precision : 0.990909090909091, Recall : 0.990909090909091, F-score : 0.990909090909091\n",
      "Iteration #3: Accuracy : 0.9867394695787831, Precision : 0.9874074074074074, Recall : 0.9874074074074074, F-score : 0.9874074074074074\n",
      "Iteration #4: Accuracy : 0.9906396255850234, Precision : 0.9905660377358491, Recall : 0.9905660377358491, F-score : 0.9905660377358491\n",
      "Iteration #5: Accuracy : 0.9024960998439937, Precision : 0.9227441285537701, Recall : 0.9227441285537701, F-score : 0.9227441285537701\n",
      "Accuracy: 0.9719207006783778\n",
      "Precision: 0.9760560439650965\n",
      "Recall: 0.970595297507133\n",
      "F1-measure: 0.9713649438261728\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5)\n",
    "GBmodel = training(X_train_b,np.array(y_train_b),gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #1: Accuracy : 0.9883086515978177, Precision : 0.9886535552193646, Recall : 0.9886535552193646, F-score : 0.9886535552193646\n",
      "Iteration #2: Accuracy : 0.9914196567862714, Precision : 0.990909090909091, Recall : 0.990909090909091, F-score : 0.990909090909091\n",
      "Iteration #3: Accuracy : 0.9867394695787831, Precision : 0.9874074074074074, Recall : 0.9874074074074074, F-score : 0.9874074074074074\n",
      "Iteration #4: Accuracy : 0.9906396255850234, Precision : 0.9905660377358491, Recall : 0.9905660377358491, F-score : 0.9905660377358491\n",
      "Iteration #5: Accuracy : 0.9024960998439937, Precision : 0.9227441285537701, Recall : 0.9227441285537701, F-score : 0.9227441285537701\n",
      "Accuracy: 0.9719207006783778\n",
      "Precision: 0.9760560439650965\n",
      "Recall: 0.970595297507133\n",
      "F1-measure: 0.9713649438261728\n"
     ]
    }
   ],
   "source": [
    "### SVM\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "svm = SVC(gamma='scale')\n",
    "svmmodel= training(X_train_b,np.array(y_train_b),svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #1: Accuracy : 0.9883086515978177, Precision : 0.9886535552193646, Recall : 0.9886535552193646, F-score : 0.9886535552193646\n",
      "Iteration #2: Accuracy : 0.9914196567862714, Precision : 0.990909090909091, Recall : 0.990909090909091, F-score : 0.990909090909091\n",
      "Iteration #3: Accuracy : 0.9867394695787831, Precision : 0.9874074074074074, Recall : 0.9874074074074074, F-score : 0.9874074074074074\n",
      "Iteration #4: Accuracy : 0.9906396255850234, Precision : 0.9905660377358491, Recall : 0.9905660377358491, F-score : 0.9905660377358491\n",
      "Iteration #5: Accuracy : 0.9024960998439937, Precision : 0.9227441285537701, Recall : 0.9227441285537701, F-score : 0.9227441285537701\n",
      "Accuracy: 0.9719207006783778\n",
      "Precision: 0.9760560439650965\n",
      "Recall: 0.970595297507133\n",
      "F1-measure: 0.9713649438261728\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=30)\n",
    "rfModel=training(X_train_b,np.array(y_train_b),rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #1: Accuracy : 0.9883086515978177, Precision : 0.9886535552193646, Recall : 0.9886535552193646, F-score : 0.9886535552193646\n",
      "Iteration #2: Accuracy : 0.9914196567862714, Precision : 0.990909090909091, Recall : 0.990909090909091, F-score : 0.990909090909091\n",
      "Iteration #3: Accuracy : 0.9867394695787831, Precision : 0.9874074074074074, Recall : 0.9874074074074074, F-score : 0.9874074074074074\n",
      "Iteration #4: Accuracy : 0.9906396255850234, Precision : 0.9905660377358491, Recall : 0.9905660377358491, F-score : 0.9905660377358491\n",
      "Iteration #5: Accuracy : 0.9024960998439937, Precision : 0.9227441285537701, Recall : 0.9227441285537701, F-score : 0.9227441285537701\n",
      "Accuracy: 0.9719207006783778\n",
      "Precision: 0.9760560439650965\n",
      "Recall: 0.970595297507133\n",
      "F1-measure: 0.9713649438261728\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('logreg', logreg), ('nb', nb), ('gb', gb), ('svm', svm), ('rf', rf)])\n",
    "eclfModel=training(X_train_b,np.array(y_train_b),eclf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #1: Accuracy : 0.9883086515978177, Precision : 0.9886535552193646, Recall : 0.9886535552193646, F-score : 0.9886535552193646\n",
      "Iteration #2: Accuracy : 0.9914196567862714, Precision : 0.990909090909091, Recall : 0.990909090909091, F-score : 0.990909090909091\n",
      "Iteration #3: Accuracy : 0.9867394695787831, Precision : 0.9874074074074074, Recall : 0.9874074074074074, F-score : 0.9874074074074074\n",
      "Iteration #4: Accuracy : 0.9906396255850234, Precision : 0.9905660377358491, Recall : 0.9905660377358491, F-score : 0.9905660377358491\n",
      "Iteration #5: Accuracy : 0.9024960998439937, Precision : 0.9227441285537701, Recall : 0.9227441285537701, F-score : 0.9227441285537701\n",
      "Accuracy: 0.9719207006783778\n",
      "Precision: 0.9760560439650965\n",
      "Recall: 0.970595297507133\n",
      "F1-measure: 0.9713649438261728\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "cart = DecisionTreeClassifier()\n",
    "num_trees = 100\n",
    "bc = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=7)\n",
    "bcModel=training(X_train_b,np.array(y_train_b),bc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a function to test the model on the test dataset and print performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testify(x_test,y_test,model):\n",
    "    y_predict = model.predict(x_test)\n",
    "    acc = accuracy_score(y_test,y_predict)\n",
    "    f1 = f1_score(y_test, y_predict, average = 'macro')\n",
    "    print('acc:',acc)\n",
    "    print('f1:',f1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing all the above models with the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.7541666666666667\n",
      "f1: 0.585855926998333\n",
      "acc: 0.7416666666666667\n",
      "f1: 0.560854680675245\n",
      "acc: 0.7458333333333333\n",
      "f1: 0.6118666984808717\n",
      "acc: 0.8791666666666667\n",
      "f1: 0.6133118506583699\n",
      "acc: 0.8583333333333333\n",
      "f1: 0.7013614404918753\n",
      "acc: 0.8625\n",
      "f1: 0.6500375591003491\n",
      "acc: 0.825\n",
      "f1: 0.696969696969697\n"
     ]
    }
   ],
   "source": [
    "# Import Test_data.\n",
    "testdata = pd.read_csv('C:/Users/wjaya/Downloads/OLIDv1.0/testset-levelb.tsv', sep = '\\t',engine='python',encoding = 'utf-8-sig')\n",
    "testdata = testdata.fillna('NULL')\n",
    "testdata = script_preprocessing(testdata)\n",
    "x_test = TFIDFmatrix(testdata['pos_lemmatized'],word_features)\n",
    "y_test = pd.read_csv('C:/Users/wjaya/Downloads/OLIDv1.0/labels-levelb.csv',sep = ',',engine='python',header = None,encoding = 'utf-8-sig')\n",
    "y_test = y_test[1].apply(lambda x:label_to_number[x])\n",
    "\n",
    "\n",
    "testify(x_test,y_test,LRmodel)\n",
    "testify(x_test,y_test,NBmodel)\n",
    "testify(x_test,y_test,GBmodel)\n",
    "testify(x_test,y_test,svmmodel)\n",
    "testify(x_test,y_test,rfModel)\n",
    "testify(x_test,y_test,eclfModel)\n",
    "testify(x_test,y_test,bcModel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model: Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model is Random Forest Classifier\n",
      "acc: 0.8583333333333333\n",
      "f1: 0.7013614404918753\n"
     ]
    }
   ],
   "source": [
    "best_model=rfModel\n",
    "y_predict = best_model.predict(x_test)\n",
    "acc = accuracy_score(y_test,y_predict)\n",
    "f1 = f1_score(y_test, y_predict, average = 'macro')\n",
    "print('Best model is Random Forest Classifier')\n",
    "print('acc:',acc)\n",
    "print('f1:',f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with a random tweet by Donald Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Red Hen Restaurant should focus more on cleaning its filthy canopies, doors and windows (badly needs a paint job) rather than refusing to serve a fine person like Sarah Huckabee Sanders. I always had a rule, if a restaurant is dirty on the outside, it is dirty on the inside!\n"
     ]
    }
   ],
   "source": [
    "#test tweet online:\n",
    "tweet=\"The Red Hen Restaurant should focus more on cleaning its filthy canopies, doors and windows (badly needs a paint job) rather than refusing to serve a fine person like Sarah Huckabee Sanders. I always had a rule, if a restaurant is dirty on the outside, it is dirty on the inside!\"\n",
    "print(tweet)\n",
    "test_tweet=np.array([tweet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(test_tweet )\n",
    "df.columns=['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = script_preprocessing(df)\n",
    "x_test = TFIDFmatrix(df['pos_lemmatized'],word_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = best_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This tweet is categorized as hate speech since it is targeted:\n",
    "The tweet is attacking The Red Hen Restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targeted\n"
     ]
    }
   ],
   "source": [
    "if int(y_predict)==0:\n",
    "    print('not targeted')\n",
    "else: print('targeted')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
